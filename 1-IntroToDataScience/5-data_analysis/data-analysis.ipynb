{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are statistical significance tests useful?\n",
    "\n",
    "* They provide a formalized framework for comparing and evaluating data\n",
    "* They enable us to evaluate whether perceived effects in our dataset reflect differences across the whole population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution (Gaussian Distribution, Bell Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two parameters associated:\n",
    "\n",
    "* Mean $$\\mu$$\n",
    "* Standard deviation $$\\sigma$$\n",
    "\n",
    "\n",
    "These two parameters plug in to the following probability density function, which describes a Gaussian distribution:\n",
    "\n",
    "![title](img/normal-d.jpg)\n",
    "\n",
    "$$f(x) = \\frac{1}{{\\sqrt {2\\pi \\sigma^2} }}e^{ - \\frac{{(x - \\mu)^2}}{2\\sigma^2}}$$\n",
    "\n",
    "* The expected <b>value of a variable described</b> by a Gaussian distribution is the <b>mean</b> and the <b>variance</b> is the <b>standard deviation</b>.\n",
    "\n",
    "* Normal distributions are also symetric about their mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-Test\n",
    "One of the most common parametric test that we can use to compare two sets of data.\n",
    "\n",
    "* Aims at accepting or rejecting a <b>null hypothesis</b>: generally a statement that we are trying to disprove by running our test)\n",
    "\n",
    "<b>TEST STATISTIC:</b> reduces the dataset to one number that helps to accept or reject the <b>null hypothesis</b>. When performing a t-Test, we compute a test statistic called <b>T</b>: \n",
    "\n",
    "$$ tTest \\rightarrow t $$\n",
    "\n",
    "Depending on the value of the test statistic T we can determine whether or not a null hypotesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Sample t-Test\n",
    "A few different versions depending on assumptions:\n",
    "* Equal sample size?\n",
    "* Same variance?\n",
    "\n",
    "$$t = \\frac{\\mu_1 - \\mu_2}{{\\sqrt {\\frac {\\sigma_1^2}{N_1} + \\frac {\\sigma_2^2}{N_2} }}}$$\n",
    "\n",
    "Where:\n",
    "* Sample mean for i sample: $$\\mu_i$$ \n",
    "* Sample variance for i'th sample: $$\\sigma_i^2$$\n",
    "* Sample size for i sample: $$N_i$$\n",
    "\n",
    "To estimate the number of degrees of freedom:\n",
    "$$\\nu \\approx \\frac{(\\frac{\\sigma_1^2}{N_1}+\\frac{\\sigma_2^2}{N_2})^2}{\\frac{\\sigma_1^4}{N_1^2 \\nu_1}+\\frac{\\sigma_2^4}{N_2^2 \\nu_2}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\nu_i = N_i - 1$$\n",
    "\n",
    "is the degrees of freedom associated with the i'th variance estimate.\n",
    "\n",
    "With these two values we can estimate the P value which is the probability of obtaining the test statistic at least as extreme as the one that was actually observed assumin that the null hypothesis was true (the P value IS NOT the probability of the null hypothesis is true given the data).\n",
    "\n",
    "* P-value: probability of obtaining a test statistic <b>at least</b> as extreme as ours if null hypothesis was true\n",
    "* Set Pcritical -> if P < Pcritical: REJECT NULL HYPOTHESIS else CANNOT REJECT NULL HYPOTHESIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Test in Python: SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-2.1004201260420148, pvalue=0.05583466515003168)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two sets of data\n",
    "lst1 = [1,2,3,4,5,6]\n",
    "lst2 = [5,4,3,2,6,7,8,9,10]\n",
    "# assumes a two-sided t-test\n",
    "scipy.stats.ttest_ind(lst1, lst2, equal_var=False)\n",
    "# returns a tuple: (t-value, p-value for a two-tailed test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For one-sided: half of two sided p-value (one side of the distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ > Mean \\rightarrow \\frac{P}{2} < P_{critical}, t > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ < Mean \\rightarrow \\frac{P}{2} < P_{critical}, t < 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 Quiz: Welch's t-Test Exercise\n",
    "Perform a t-test on two sets of baseball data (left-handed and right-handed hitters).\n",
    "\n",
    "Receive a csv file that has three columns.  A player's name, handedness (L for lefthanded or R for righthanded) and their career batting average (called 'avg'). \n",
    "    \n",
    "Read that the csv file into a pandas data frame, and run Welch's t-test on the two cohorts defined by handedness.\n",
    "\n",
    "One cohort should be a data frame of right-handed batters. And the other cohort should be a data frame of left-handed batters.\n",
    "    \n",
    "* With a significance level of 95%, if there is no difference between the two cohorts, return a tuple consisting of True, and then the tuple returned by scipy.stats.ttest.  \n",
    "    \n",
    "* If there is a difference, return a tuple consisting of False, and then the tuple returned by scipy.stats.ttest.\n",
    "    \n",
    "For example, the tuple that you return may look like:\n",
    "* (True, (9.93570222, 0.000023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_dir():\n",
    "    return os.getcwd() + '/data/input/'\n",
    "\n",
    "def output_dir():\n",
    "    return os.getcwd() + '/data/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv_data(filename, input_dir):\n",
    "    '''\n",
    "    Receives a file name (csv)\n",
    "    Returns a DataFrame\n",
    "    '''\n",
    "    data = pd.read_csv(input_dir + filename)\n",
    "    \n",
    "    #Rename the columns by replacing spaces with underscores and setting all characters to lowercase\n",
    "    data.rename(columns = lambda x: x.replace(' ', '_').lower(), inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t_test_compare(lst1, lst2 ):\n",
    "    \"\"\"\n",
    "    Compare averages\n",
    "    Performs a t-test on two sets of average data\n",
    "    \"\"\"\n",
    "    t_test_tuple = sps.ttest_ind(lst1, lst2, equal_var=False)\n",
    "    tvalue = t_test_tuple.statistic\n",
    "    pvalue = t_test_tuple.pvalue\n",
    "    # ex: pvalue = 5% -> there is a 5% chance of finding a difference (probability of rejecting the null hypothesis when it is true)\n",
    "    # as large as (or larger than) the one in our study given that the null hypothesis is true\n",
    "    # A low P value suggests that the sample provides enough evidence that we can reject \n",
    "    # the null hypothesis for the entire population.\n",
    "    # pvalue tells the strength of the evidence\n",
    "\n",
    "\n",
    "    # With a significance level of 95% \n",
    "    if pvalue >= 0.05:\n",
    "        # No difference\n",
    "        return (True, (tvalue,pvalue))\n",
    "    else:\n",
    "        # There is a difference\n",
    "        return (False, (tvalue,pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseball_data = read_csv_data('baseball-data.csv',input_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name handedness height weight    avg   hr\n",
      "0           Brandon Hyde          R     75    210  0.000    0\n",
      "1            Carey Selph          R     69    175  0.277    0\n",
      "2           Philip Nastu          L     74    180  0.040    0\n",
      "3             Kent Hrbek          L     76    200  0.282  293\n",
      "4            Bill Risley          R     74    215  0.000    0\n",
      "5                   Wood        NaN                0.000    0\n",
      "6        Steve Gajkowski          R     74    200  0.000    0\n",
      "7              Rick Schu          R     72    170  0.246   41\n",
      "8              Tom Brown          R     73    170  0.000    0\n",
      "9           Tom Browning          L     73    190  0.153    2\n",
      "10           Tommy Brown          R     73    170  0.241   31\n",
      "11             Tom Brown          B     73    190  0.147    1\n",
      "12              Joe Burg          R     70    143  0.326    0\n",
      "13             Tom Brown          L     70    168  0.265   64\n",
      "14         Terry McGriff          R     74    190  0.206    3\n",
      "15       Floyd Bannister          L     73    190  0.175    0\n",
      "16       Richard Hidalgo          R     75    220  0.269  171\n",
      "17        Denny Lemaster          R     73    182  0.130    4\n",
      "18         George Stultz        NaN     70    150  0.333    0\n",
      "19             Joe Nolan          L     71    175  0.263   27\n",
      "20           Paul Hinson          R     70    150  0.000    0\n",
      "21          George Young          L     72    185  0.000    0\n",
      "22          Gerald Young          B     74    185  0.246    3\n",
      "23            Tim Talton          L     75    200  0.295    2\n",
      "24        Johnny Schaive          R     68    175  0.232    6\n",
      "25         Martin Mullen        NaN                0.000    0\n",
      "26       Wally Reinecker          R     66    150  0.125    0\n",
      "27            Russ Kerns          L     72    188  0.000    0\n",
      "28           Chris Gwynn          L     72    200  0.261   17\n",
      "29           Joe Kraemer          L     74    185  0.000    0\n",
      "...                  ...        ...    ...    ...    ...  ...\n",
      "18147        Lloyd Allen          R     73    185  0.200    1\n",
      "18148       Walt Bashore          R     72    170  0.200    0\n",
      "18149      Bill Van Dyke          R     68    170  0.253    2\n",
      "18150        Jeff Liefer          L     75    195  0.230   31\n",
      "18151         Gene Baker          R     73    170  0.265   39\n",
      "18152  Kristopher Negron          R     72    195  0.250    0\n",
      "18153          Al Kaline          R     73    175  0.297  399\n",
      "18154       George Baker        NaN     71    162  0.156    0\n",
      "18155       Rocky Nelson          L     70    175  0.249   31\n",
      "18156          Alex Diaz          B     71    175  0.239    8\n",
      "18157       Bobby Kielty          B     73    225  0.254   53\n",
      "18158          Jim Otten          R     74    195  0.143    0\n",
      "18159       Todd Fischer          R     70    170  0.000    0\n",
      "18160      Bill Killefer          R     70    170  0.238    4\n",
      "18161     Clarence Huber          R     70    165  0.263    6\n",
      "18162        Brick Smith          R     76    225  0.111    0\n",
      "18163        Brian Smith          R     71    185  0.000    0\n",
      "18164    Manny Delcarmen          R     74    205  0.000    0\n",
      "18165        Rudy Kallio          R     70    160  0.167    0\n",
      "18166          Ed Barney          L     70    178  0.224    0\n",
      "18167         Mike Joyce          R     74    193  0.429    0\n",
      "18168        Joe Blanton          R     75    245  0.108    0\n",
      "18169        Jeremy Hill          R     71    200  0.000    0\n",
      "18170      Aurelio Lopez          R     72    185  0.125    0\n",
      "18171         Jesse Hill          R     69    165  0.289    6\n",
      "18172         Rob Nelson          L     76    215  0.178    4\n",
      "18173       Roger Nelson          R     75    200  0.128    0\n",
      "18174       Fred Luderus          L     71    185  0.277   84\n",
      "18175           Ed Glenn          R     70    160  0.202    1\n",
      "18176           Ed Glenn          R                0.067    0\n",
      "\n",
      "[18177 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(baseball_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right_h = baseball_data[baseball_data['handedness'] == 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_h = baseball_data[baseball_data['handedness'] == 'L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, (-9.9357022262420944, 3.8102742258887383e-23))\n"
     ]
    }
   ],
   "source": [
    "# Ignoring NaN handness\n",
    "t_test_result = t_test_compare(right_h['avg'], left_h['avg'])\n",
    "print(t_test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric Test\n",
    "Statistical test that does not assume our data is drawn from anny particular underlying probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann-Whitney U Test (-Wilcoxan Test)\n",
    "This is a test of the null hypothesis that two populations are the same.\n",
    "\n",
    "Tests whether or not these samples came from the same population - but not necessarily which one has a higher mean or higher median or anything like that\n",
    "\n",
    "Because of this it is usually useful to report Mann-Whitney U Test results along with some other information (like the two samples means, or the sample medians...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannwhitneyuResult(statistic=22523894.5, pvalue=3.7307870396512496e-45)\n"
     ]
    }
   ],
   "source": [
    "# u: Mann-Whitney test statistic\n",
    "# p: one sided pvalue\n",
    "u_test = sps.mannwhitneyu(right_h['avg'], left_h['avg'])\n",
    "print(u_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "A branch of artificial intelligence focused on constructing systems that learn from large amounts of data to make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics vs. Machine Learning\n",
    "NOT MUCH\n",
    "\n",
    "* Statistics is focused on analyzing existing data, and drawing valid conclusions (care about how the data is collected and drawing conclusions about that existing data using probability models)\n",
    "* Machine Learning is focused on making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Machine Learning: Supervised and Unsupervised\n",
    "Data -> MODEL -> Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "Do not have any such training examples. Instead, we have a bunch of unlabeled data points and we are trying to understand the structure of the data, often by clustering similar data points together.\n",
    "* Trying to understand structure of data\n",
    "* Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Supervised Learning\n",
    " There are labeled inputs that we train the model on. Training the model means teaching the model what the correct answer looks like.\n",
    " * Have examples with input and output\n",
    " * Predict output for future\n",
    " * Classification\n",
    " * Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with Gradient Descent\n",
    "Can we write an equation that takes a bunch of info (e.g., height, weight, birth year, position) and predicts the number of home runs? Yeah, regression!\n",
    "\n",
    "Each data point (1..m) has an output variable Y and n input variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\n",
    "(1)\\begin{bmatrix}\n",
    "    Y\\\\\n",
    "    x_{1}\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    x_{n} \n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\n",
    "(2)\\begin{bmatrix}\n",
    "    Y\\\\\n",
    "    x_{1}\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    x_{n} \n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\n",
    "(3)\\begin{bmatrix}\n",
    "    Y\\\\\n",
    "    x_{1}\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    x_{n} \n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\n",
    "\\dots\n",
    "$\n",
    "$\n",
    "(m)\\begin{bmatrix}\n",
    "    Y\\\\\n",
    "    x_{1}\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    x_{n} \n",
    "\\end{bmatrix}\n",
    "$\n",
    "$\n",
    "\\theta_1 \\dots \\theta_n\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the baseball example Y is the lifetime number of home runs and x1..xn are parameters like height and weight; one through m samples might be diffent baseball players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to predict the values of the output variable for each data point by multiplying the input variables by some set of coefficients (theta 1 through theta N). Each theta <b>tells how important</b> an input variable is when predicting a value for the output variable. If theta 1 is <b>very small</b> then x1 <b>must not be very important</b> in general when predicting Y (and if theta n is very large then Xn is generally a big contributor for the value of Y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is build in such a way that we can muntiply each X by the corresponding theta and sum them up to get Y. So the final equation will look something like this:\n",
    "\n",
    "$\\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n = Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best equation is the one that's is going to minimize the difference across all data points between our predicted Y and our observed Y. To find this equation we need to find the thetas that produce the best predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a value that describes the total errors of the model: \n",
    "\n",
    "$\\sum_{m}^{i=1}{(Y_{predicted} - Y_{actual})^2}$\n",
    "\n",
    "However since these errors can be both negative and positive if we simply sum them up we could have a total error term that is very close to 0 even if the model is very wrong. That is why we need to add the square of the error terms (the magnitude of each individual error term will be positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to find theta values: Gradient Descent\n",
    "1) Define a cost function $J(\\theta)$ where $\\theta$ here is used to represent the entire set of thetas\n",
    "\n",
    "The cost function is meant to provide a measure of how well the current set of thetas does at modeling the observed data so we want to minimize the cost function's value.\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}{(h(X^i) - Y^i)^2}$ \n",
    "\n",
    "where\n",
    "\n",
    "$h(X^i) = \\sum_{n}^{j=1}{\\theta_jX_{j}^{i}} = \\theta_0X_{0}^{i} + \\theta_1X_{1}^{i} + \\dots + \\theta_nX_{n}^{i} = Y_{predicted}^{i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to find the correct values of theta to minimize the cost function $J(\\theta)$? Use a <b>search algorithm</b> that takes some initial guess for a theta and iteratively changes theta so that $J(\\theta)$ keeps on getting smaller until it convergers on some minimum value (<b>gradient descent</b>). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 Quiz: Gradient Descent in Python\n",
    "Perform gradient descent given a data set with an arbitrary number of features.\n",
    "\n",
    "Useful formula:\n",
    "\n",
    "$\\theta_j = \\theta_j + \\frac{\\alpha}{m} \\sum_{i=1}^{m}{(Y^i - h(X^i))X^{i}_{j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(features, values, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost of a list of parameters, theta, given a list of features \n",
    "    (input data points) and values (output data points).\n",
    "    \"\"\"\n",
    "    m = len(values)\n",
    "    sum_of_square_errors = np.square(np.dot(features, theta) - values).sum()\n",
    "    cost = sum_of_square_errors / (2*m)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(features, values, theta, alpha, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent given a data set with an arbitrary number of features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Write code here that performs num_iterations updates to the elements of theta.\n",
    "    # times. Every time you compute the cost for a given list of thetas, append it \n",
    "    # to cost_history.\n",
    "    # See the Instructor notes for hints. \n",
    "    \n",
    "    m = len(values)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        print (i,cost_history)\n",
    "        # update values\n",
    "        # dot product of the features and theta\n",
    "        predicted_values = np.dot(features, theta)\n",
    "        # update theta\n",
    "        #print (predicted_values)\n",
    "        theta = theta - alpha / m * np.dot((predicted_values - values), features)\n",
    "        \n",
    "        cost = compute_cost(features, values, theta.transpose())\n",
    "        cost_history.append(cost)\n",
    "    \n",
    "    return theta, pd.Series(cost_history)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name handedness height weight    avg   hr\n",
      "0           Brandon Hyde          R     75    210  0.000    0\n",
      "1            Carey Selph          R     69    175  0.277    0\n",
      "2           Philip Nastu          L     74    180  0.040    0\n",
      "3             Kent Hrbek          L     76    200  0.282  293\n",
      "4            Bill Risley          R     74    215  0.000    0\n",
      "5                   Wood        NaN                0.000    0\n",
      "6        Steve Gajkowski          R     74    200  0.000    0\n",
      "7              Rick Schu          R     72    170  0.246   41\n",
      "8              Tom Brown          R     73    170  0.000    0\n",
      "9           Tom Browning          L     73    190  0.153    2\n",
      "10           Tommy Brown          R     73    170  0.241   31\n",
      "11             Tom Brown          B     73    190  0.147    1\n",
      "12              Joe Burg          R     70    143  0.326    0\n",
      "13             Tom Brown          L     70    168  0.265   64\n",
      "14         Terry McGriff          R     74    190  0.206    3\n",
      "15       Floyd Bannister          L     73    190  0.175    0\n",
      "16       Richard Hidalgo          R     75    220  0.269  171\n",
      "17        Denny Lemaster          R     73    182  0.130    4\n",
      "18         George Stultz        NaN     70    150  0.333    0\n",
      "19             Joe Nolan          L     71    175  0.263   27\n",
      "20           Paul Hinson          R     70    150  0.000    0\n",
      "21          George Young          L     72    185  0.000    0\n",
      "22          Gerald Young          B     74    185  0.246    3\n",
      "23            Tim Talton          L     75    200  0.295    2\n",
      "24        Johnny Schaive          R     68    175  0.232    6\n",
      "25         Martin Mullen        NaN                0.000    0\n",
      "26       Wally Reinecker          R     66    150  0.125    0\n",
      "27            Russ Kerns          L     72    188  0.000    0\n",
      "28           Chris Gwynn          L     72    200  0.261   17\n",
      "29           Joe Kraemer          L     74    185  0.000    0\n",
      "...                  ...        ...    ...    ...    ...  ...\n",
      "18147        Lloyd Allen          R     73    185  0.200    1\n",
      "18148       Walt Bashore          R     72    170  0.200    0\n",
      "18149      Bill Van Dyke          R     68    170  0.253    2\n",
      "18150        Jeff Liefer          L     75    195  0.230   31\n",
      "18151         Gene Baker          R     73    170  0.265   39\n",
      "18152  Kristopher Negron          R     72    195  0.250    0\n",
      "18153          Al Kaline          R     73    175  0.297  399\n",
      "18154       George Baker        NaN     71    162  0.156    0\n",
      "18155       Rocky Nelson          L     70    175  0.249   31\n",
      "18156          Alex Diaz          B     71    175  0.239    8\n",
      "18157       Bobby Kielty          B     73    225  0.254   53\n",
      "18158          Jim Otten          R     74    195  0.143    0\n",
      "18159       Todd Fischer          R     70    170  0.000    0\n",
      "18160      Bill Killefer          R     70    170  0.238    4\n",
      "18161     Clarence Huber          R     70    165  0.263    6\n",
      "18162        Brick Smith          R     76    225  0.111    0\n",
      "18163        Brian Smith          R     71    185  0.000    0\n",
      "18164    Manny Delcarmen          R     74    205  0.000    0\n",
      "18165        Rudy Kallio          R     70    160  0.167    0\n",
      "18166          Ed Barney          L     70    178  0.224    0\n",
      "18167         Mike Joyce          R     74    193  0.429    0\n",
      "18168        Joe Blanton          R     75    245  0.108    0\n",
      "18169        Jeremy Hill          R     71    200  0.000    0\n",
      "18170      Aurelio Lopez          R     72    185  0.125    0\n",
      "18171         Jesse Hill          R     69    165  0.289    6\n",
      "18172         Rob Nelson          L     76    215  0.178    4\n",
      "18173       Roger Nelson          R     75    200  0.128    0\n",
      "18174       Fred Luderus          L     71    185  0.277   84\n",
      "18175           Ed Glenn          R     70    160  0.202    1\n",
      "18176           Ed Glenn          R                0.067    0\n",
      "\n",
      "[18177 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "baseball_data = read_csv_data('baseball-data.csv',input_dir())\n",
    "print(baseball_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate features and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = baseball_data[['height', 'weight']]\n",
    "values = baseball_data[['hr']]\n",
    "# m = number of data points\n",
    "m = len(values)\n",
    "# print (features.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace empty entries with np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_df_empty_entries(df):\n",
    "    return df.replace(r'\\s+', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform series to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_to_numeric(s):\n",
    "    return pd.to_numeric(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale to Unit Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_unit_lenght(s):\n",
    "    return s / (s.max() - s.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_normalization(s):\n",
    "    return (s - s.mean()) / (s.max() - s.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       height    weight\n",
      "0       1.875  0.823529\n",
      "1       1.725  0.686275\n",
      "2       1.850  0.705882\n",
      "3       1.900  0.784314\n",
      "4       1.850  0.843137\n",
      "6       1.850  0.784314\n",
      "7       1.800  0.666667\n",
      "8       1.825  0.666667\n",
      "9       1.825  0.745098\n",
      "10      1.825  0.666667\n",
      "11      1.825  0.745098\n",
      "12      1.750  0.560784\n",
      "13      1.750  0.658824\n",
      "14      1.850  0.745098\n",
      "15      1.825  0.745098\n",
      "16      1.875  0.862745\n",
      "17      1.825  0.713725\n",
      "18      1.750  0.588235\n",
      "19      1.775  0.686275\n",
      "20      1.750  0.588235\n",
      "21      1.800  0.725490\n",
      "22      1.850  0.725490\n",
      "23      1.875  0.784314\n",
      "24      1.700  0.686275\n",
      "26      1.650  0.588235\n",
      "27      1.800  0.737255\n",
      "28      1.800  0.784314\n",
      "29      1.850  0.725490\n",
      "30      1.800  0.666667\n",
      "31      1.900  0.764706\n",
      "...       ...       ...\n",
      "18146   1.775  0.654902\n",
      "18147   1.825  0.725490\n",
      "18148   1.800  0.666667\n",
      "18149   1.700  0.666667\n",
      "18150   1.875  0.764706\n",
      "18151   1.825  0.666667\n",
      "18152   1.800  0.764706\n",
      "18153   1.825  0.686275\n",
      "18154   1.775  0.635294\n",
      "18155   1.750  0.686275\n",
      "18156   1.775  0.686275\n",
      "18157   1.825  0.882353\n",
      "18158   1.850  0.764706\n",
      "18159   1.750  0.666667\n",
      "18160   1.750  0.666667\n",
      "18161   1.750  0.647059\n",
      "18162   1.900  0.882353\n",
      "18163   1.775  0.725490\n",
      "18164   1.850  0.803922\n",
      "18165   1.750  0.627451\n",
      "18166   1.750  0.698039\n",
      "18167   1.850  0.756863\n",
      "18168   1.875  0.960784\n",
      "18169   1.775  0.784314\n",
      "18170   1.800  0.725490\n",
      "18171   1.725  0.647059\n",
      "18172   1.900  0.843137\n",
      "18173   1.875  0.784314\n",
      "18174   1.775  0.725490\n",
      "18175   1.750  0.627451\n",
      "\n",
      "[17317 rows x 2 columns]\n",
      "       height    weight\n",
      "0       1.875  0.097842\n",
      "1       1.725 -0.039413\n",
      "2       1.850 -0.019805\n",
      "3       1.900  0.058626\n",
      "4       1.850  0.117450\n",
      "6       1.850  0.058626\n",
      "7       1.800 -0.059021\n",
      "8       1.825 -0.059021\n",
      "9       1.825  0.019410\n",
      "10      1.825 -0.059021\n",
      "11      1.825  0.019410\n",
      "12      1.750 -0.164903\n",
      "13      1.750 -0.066864\n",
      "14      1.850  0.019410\n",
      "15      1.825  0.019410\n",
      "16      1.875  0.137057\n",
      "17      1.825 -0.011962\n",
      "18      1.750 -0.137452\n",
      "19      1.775 -0.039413\n",
      "20      1.750 -0.137452\n",
      "21      1.800 -0.000197\n",
      "22      1.850 -0.000197\n",
      "23      1.875  0.058626\n",
      "24      1.700 -0.039413\n",
      "26      1.650 -0.137452\n",
      "27      1.800  0.011567\n",
      "28      1.800  0.058626\n",
      "29      1.850 -0.000197\n",
      "30      1.800 -0.059021\n",
      "31      1.900  0.039018\n",
      "...       ...       ...\n",
      "18146   1.775 -0.070786\n",
      "18147   1.825 -0.000197\n",
      "18148   1.800 -0.059021\n",
      "18149   1.700 -0.059021\n",
      "18150   1.875  0.039018\n",
      "18151   1.825 -0.059021\n",
      "18152   1.800  0.039018\n",
      "18153   1.825 -0.039413\n",
      "18154   1.775 -0.090394\n",
      "18155   1.750 -0.039413\n",
      "18156   1.775 -0.039413\n",
      "18157   1.825  0.156665\n",
      "18158   1.850  0.039018\n",
      "18159   1.750 -0.059021\n",
      "18160   1.750 -0.059021\n",
      "18161   1.750 -0.078629\n",
      "18162   1.900  0.156665\n",
      "18163   1.775 -0.000197\n",
      "18164   1.850  0.078234\n",
      "18165   1.750 -0.098237\n",
      "18166   1.750 -0.027648\n",
      "18167   1.850  0.031175\n",
      "18168   1.875  0.235097\n",
      "18169   1.775  0.058626\n",
      "18170   1.800 -0.000197\n",
      "18171   1.725 -0.078629\n",
      "18172   1.900  0.117450\n",
      "18173   1.875  0.058626\n",
      "18174   1.775 -0.000197\n",
      "18175   1.750 -0.098237\n",
      "\n",
      "[17317 rows x 2 columns]\n",
      "       height    weight\n",
      "0       1.875  0.097842\n",
      "1       1.725 -0.039413\n",
      "2       1.850 -0.019805\n",
      "3       1.900  0.058626\n",
      "4       1.850  0.117450\n",
      "6       1.850  0.058626\n",
      "7       1.800 -0.059021\n",
      "8       1.825 -0.059021\n",
      "9       1.825  0.019410\n",
      "10      1.825 -0.059021\n",
      "11      1.825  0.019410\n",
      "12      1.750 -0.164903\n",
      "13      1.750 -0.066864\n",
      "14      1.850  0.019410\n",
      "15      1.825  0.019410\n",
      "16      1.875  0.137057\n",
      "17      1.825 -0.011962\n",
      "18      1.750 -0.137452\n",
      "19      1.775 -0.039413\n",
      "20      1.750 -0.137452\n",
      "21      1.800 -0.000197\n",
      "22      1.850 -0.000197\n",
      "23      1.875  0.058626\n",
      "24      1.700 -0.039413\n",
      "26      1.650 -0.137452\n",
      "27      1.800  0.011567\n",
      "28      1.800  0.058626\n",
      "29      1.850 -0.000197\n",
      "30      1.800 -0.059021\n",
      "31      1.900  0.039018\n",
      "...       ...       ...\n",
      "18146   1.775 -0.070786\n",
      "18147   1.825 -0.000197\n",
      "18148   1.800 -0.059021\n",
      "18149   1.700 -0.059021\n",
      "18150   1.875  0.039018\n",
      "18151   1.825 -0.059021\n",
      "18152   1.800  0.039018\n",
      "18153   1.825 -0.039413\n",
      "18154   1.775 -0.090394\n",
      "18155   1.750 -0.039413\n",
      "18156   1.775 -0.039413\n",
      "18157   1.825  0.156665\n",
      "18158   1.850  0.039018\n",
      "18159   1.750 -0.059021\n",
      "18160   1.750 -0.059021\n",
      "18161   1.750 -0.078629\n",
      "18162   1.900  0.156665\n",
      "18163   1.775 -0.000197\n",
      "18164   1.850  0.078234\n",
      "18165   1.750 -0.098237\n",
      "18166   1.750 -0.027648\n",
      "18167   1.850  0.031175\n",
      "18168   1.875  0.235097\n",
      "18169   1.775  0.058626\n",
      "18170   1.800 -0.000197\n",
      "18171   1.725 -0.078629\n",
      "18172   1.900  0.117450\n",
      "18173   1.875  0.058626\n",
      "18174   1.775 -0.000197\n",
      "18175   1.750 -0.098237\n",
      "\n",
      "[17317 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "features = replace_df_empty_entries(features)\n",
    "features['height'] = series_to_numeric(features['height'])\n",
    "features['weight'] = series_to_numeric(features['weight'])\n",
    "print(features)\n",
    "\n",
    "values = normalize_unit_lenght(values)\n",
    "features['height'] = normalize_unit_lenght(features['height'])\n",
    "features['weight'] = normalize_unit_lenght(features['weight'])\n",
    "features['weight'] = normalize_unit_lenght(features['weight'])\n",
    "\n",
    "print(features)\n",
    "\n",
    "features['height'] = pd.Series(features['height'])\n",
    "features['weight'] = pd.Series(features['weight'])\n",
    "features = pd.concat([features['height'], features['weight']], axis = 1)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[np.isfinite(features['height'])]\n",
    "features = features[np.isfinite(features['weight'])]\n",
    "#theta, cost_history = gradient_descent(features.values, values.values,[0.,0.], 0.01, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficient of Determination ($R^2$)\n",
    "One pretty effective way to evaluate the effectiveness of the model.\n",
    "\n",
    "data = $y_i \\dots y_n$\n",
    "\n",
    "predictions = $f_i \\dots f_n$\n",
    "\n",
    "average of data = $\\bar{y}$\n",
    "\n",
    "$R^2 \\equiv 1 - \\frac{\\sum_{n} (y_i - f_i)^2 }{\\sum_{n} (y_i - \\bar{y})^2}$\n",
    "\n",
    "The closer $R^2$ is to 1, the beter is the model. The closer it to zero the poorer is the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 Quiz: Calculating $R^2$\n",
    "Function to compute $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_r_squared(data, predictions):\n",
    "    '''\n",
    "    Receives numpy arrays: data and predictions and return the coefficient of determination for the model\n",
    "    '''\n",
    "    \n",
    "    d_mean = np.mean(data)\n",
    "    r_squared = 1 - np.sum((data - predictions)**2) / np.sum((data - d_mean)**2)\n",
    "    # ((data - predictions)**2).sum() is also correct\n",
    "    return r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional Considerations\n",
    "To apply linear regression in real problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Other types of linear regression\n",
    "    * Ordinary Least Squares Regression (always guaranteed to find the optimal solution when performing Linear Regression whereas Gradient Descent is not)\n",
    "* Parameter estimation\n",
    "    * What are the confidence intervals on the parameters?\n",
    "    * What is the likelihood we would calculate this parameter value if the parameter had no effect on the output variable?\n",
    "* Under/Overfitting (\n",
    "    * Is not so much a problem with linear regression\n",
    "    * Cross validation\n",
    "* Multiple Local Minima\n",
    "    * Use various different random initial thetas\n",
    "    * Seed random values for repeatability (make results replicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
