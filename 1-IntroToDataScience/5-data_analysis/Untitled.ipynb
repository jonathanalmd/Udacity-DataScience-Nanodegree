{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why are statistical significance tests useful?\n",
    "\n",
    "* They provide a formalized framework for comparing and evaluating data\n",
    "* They enable us to evaluate whether perceived effects in our dataset reflect differences across the whole population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution (Gaussian Distribution, Bell Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two parameters associated:\n",
    "\n",
    "* Mean $$\\mu$$\n",
    "* Standard deviation $$\\sigma$$\n",
    "\n",
    "\n",
    "These two parameters plug in to the following probability density function, which describes a Gaussian distribution:\n",
    "\n",
    "![title](img/normal-d.jpg)\n",
    "\n",
    "$$f(x) = \\frac{1}{{\\sqrt {2\\pi \\sigma^2} }}e^{ - \\frac{{(x - \\mu)^2}}{2\\sigma^2}}$$\n",
    "\n",
    "* The expected <b>value of a variable described</b> by a Gaussian distribution is the <b>mean</b> and the <b>variance</b> is the <b>standard deviation</b>.\n",
    "\n",
    "* Normal distributions are also symetric about their mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-Test\n",
    "One of the most common parametric test that we can use to compare two sets of data.\n",
    "\n",
    "* Aims at accepting or rejecting a <b>null hypothesis</b>: generally a statement that we are trying to disprove by running our test)\n",
    "\n",
    "<b>TEST STATISTIC:</b> reduces the dataset to one number that helps to accept or reject the <b>null hypothesis</b>. When performing a t-Test, we compute a test statistic called <b>T</b>. \n",
    "\n",
    "$$ tTest \\rightarrow t $$\n",
    "\n",
    "Depending on the value of the test statistic T we can determine whether or not a null hypotesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Sample t-Test\n",
    "A few different versions depending on assumptions:\n",
    "* Equal sample size?\n",
    "* Same variance?\n",
    "\n",
    "$$t = \\frac{\\mu_1 - \\mu_2}{{\\sqrt {\\frac {\\sigma_1^2}{N_1} + \\frac {\\sigma_2^2}{N_2} }}}$$\n",
    "\n",
    "Where:\n",
    "* Sample mean for i sample: $$\\mu_i$$ \n",
    "* Sample variance for i'th sample: $$\\sigma_i^2$$\n",
    "* Sample size for i sample: $$N_i$$\n",
    "\n",
    "To estimate the number of degrees of freedom:\n",
    "$$\\nu \\approx \\frac{(\\frac{\\sigma_1^2}{N_1}+\\frac{\\sigma_2^2}{N_2})^2}{\\frac{\\sigma_1^4}{N_1^2 \\nu_1}+\\frac{\\sigma_2^4}{N_2^2 \\nu_2}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\\nu_i = N_i - 1$$\n",
    "\n",
    "is the degrees of freedom associated with the i'th variance estimate.\n",
    "\n",
    "With these two values we can estimate the P value which is the probability of obtaining the test statistic at least as extreme as the one that was actually observed assumin that the null hypothesis was true (the P value IS NOT the probability of the null hypothesis is true given the data).\n",
    "\n",
    "* P-value: probability of obtaining a test statistic <b>at least</b> as extreme as ours if null hypothesis was true\n",
    "* Set Pcritical -> if P < Pcritical: REJECT NULL HYPOTHESIS else CANNOT REJECT NULL HYPOTHESIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Test in Python: SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-2.1004201260420148, pvalue=0.05583466515003168)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two sets of data\n",
    "lst1 = [1,2,3,4,5,6]\n",
    "lst2 = [5,4,3,2,6,7,8,9,10]\n",
    "# assumes a two-sided t-test\n",
    "scipy.stats.ttest_ind(lst1, lst2, equal_var=False)\n",
    "# returns a tuple: (t-value, p-value for a two-tailed test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For one-sided: half of two sided p-value (one side of the distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ > Mean \\rightarrow \\frac{P}{2} < P_{critical}, t > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ < Mean \\rightarrow \\frac{P}{2} < P_{critical}, t < 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 Quiz: Welch's t-Test Exercise\n",
    "Perform a t-test on two sets of baseball data (left-handed and right-handed hitters).\n",
    "\n",
    "Receive a csv file that has three columns.  A player's name, handedness (L for lefthanded or R for righthanded) and their career batting average (called 'avg'). \n",
    "    \n",
    "Read that the csv file into a pandas data frame, and run Welch's t-test on the two cohorts defined by handedness.\n",
    "\n",
    "One cohort should be a data frame of right-handed batters. And the other cohort should be a data frame of left-handed batters.\n",
    "    \n",
    "* With a significance level of 95%, if there is no difference between the two cohorts, return a tuple consisting of True, and then the tuple returned by scipy.stats.ttest.  \n",
    "    \n",
    "* If there is a difference, return a tuple consisting of False, and then the tuple returned by scipy.stats.ttest.\n",
    "    \n",
    "For example, the tuple that you return may look like:\n",
    "* (True, (9.93570222, 0.000023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_dir():\n",
    "    return os.getcwd() + '/data/input/'\n",
    "\n",
    "def output_dir():\n",
    "    return os.getcwd() + '/data/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv_data(filename, input_dir):\n",
    "    '''\n",
    "    Receives a file name (csv)\n",
    "    Returns a DataFrame\n",
    "    '''\n",
    "    data = pd.read_csv(input_dir + filename)\n",
    "    \n",
    "    #Rename the columns by replacing spaces with underscores and setting all characters to lowercase\n",
    "    data.rename(columns = lambda x: x.replace(' ', '_').lower(), inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_compare(lst1, lst2 ):\n",
    "    \"\"\"\n",
    "    Compare averages\n",
    "    Performs a t-test on two sets of average data\n",
    "    \"\"\"\n",
    "    t_test_tuple = sps.ttest_ind(lst1, lst2, equal_var=False)\n",
    "    tvalue = t_test_tuple.statistic\n",
    "    pvalue = t_test_tuple.pvalue\n",
    "    # ex: pvalue = 5% -> there is a 5% chance of finding a difference (probability of rejecting the null hypothesis when it is true)\n",
    "    # as large as (or larger than) the one in our study given that the null hypothesis is true\n",
    "    # A low P value suggests that the sample provides enough evidence that we can reject \n",
    "    # the null hypothesis for the entire population.\n",
    "    # pvalue tells the strength of the evidence\n",
    "\n",
    "\n",
    "    # With a significance level of 95% \n",
    "    if pvalue >= 0.05:\n",
    "        # No difference\n",
    "        return (True, (tvalue,pvalue))\n",
    "    else:\n",
    "        # There is a difference\n",
    "        return (False, (tvalue,pvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball_data = read_csv_data('baseball-data.csv',input_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name handedness height weight    avg   hr\n",
      "0           Brandon Hyde          R     75    210  0.000    0\n",
      "1            Carey Selph          R     69    175  0.277    0\n",
      "2           Philip Nastu          L     74    180  0.040    0\n",
      "3             Kent Hrbek          L     76    200  0.282  293\n",
      "4            Bill Risley          R     74    215  0.000    0\n",
      "5                   Wood        NaN                0.000    0\n",
      "6        Steve Gajkowski          R     74    200  0.000    0\n",
      "7              Rick Schu          R     72    170  0.246   41\n",
      "8              Tom Brown          R     73    170  0.000    0\n",
      "9           Tom Browning          L     73    190  0.153    2\n",
      "10           Tommy Brown          R     73    170  0.241   31\n",
      "11             Tom Brown          B     73    190  0.147    1\n",
      "12              Joe Burg          R     70    143  0.326    0\n",
      "13             Tom Brown          L     70    168  0.265   64\n",
      "14         Terry McGriff          R     74    190  0.206    3\n",
      "15       Floyd Bannister          L     73    190  0.175    0\n",
      "16       Richard Hidalgo          R     75    220  0.269  171\n",
      "17        Denny Lemaster          R     73    182  0.130    4\n",
      "18         George Stultz        NaN     70    150  0.333    0\n",
      "19             Joe Nolan          L     71    175  0.263   27\n",
      "20           Paul Hinson          R     70    150  0.000    0\n",
      "21          George Young          L     72    185  0.000    0\n",
      "22          Gerald Young          B     74    185  0.246    3\n",
      "23            Tim Talton          L     75    200  0.295    2\n",
      "24        Johnny Schaive          R     68    175  0.232    6\n",
      "25         Martin Mullen        NaN                0.000    0\n",
      "26       Wally Reinecker          R     66    150  0.125    0\n",
      "27            Russ Kerns          L     72    188  0.000    0\n",
      "28           Chris Gwynn          L     72    200  0.261   17\n",
      "29           Joe Kraemer          L     74    185  0.000    0\n",
      "...                  ...        ...    ...    ...    ...  ...\n",
      "18147        Lloyd Allen          R     73    185  0.200    1\n",
      "18148       Walt Bashore          R     72    170  0.200    0\n",
      "18149      Bill Van Dyke          R     68    170  0.253    2\n",
      "18150        Jeff Liefer          L     75    195  0.230   31\n",
      "18151         Gene Baker          R     73    170  0.265   39\n",
      "18152  Kristopher Negron          R     72    195  0.250    0\n",
      "18153          Al Kaline          R     73    175  0.297  399\n",
      "18154       George Baker        NaN     71    162  0.156    0\n",
      "18155       Rocky Nelson          L     70    175  0.249   31\n",
      "18156          Alex Diaz          B     71    175  0.239    8\n",
      "18157       Bobby Kielty          B     73    225  0.254   53\n",
      "18158          Jim Otten          R     74    195  0.143    0\n",
      "18159       Todd Fischer          R     70    170  0.000    0\n",
      "18160      Bill Killefer          R     70    170  0.238    4\n",
      "18161     Clarence Huber          R     70    165  0.263    6\n",
      "18162        Brick Smith          R     76    225  0.111    0\n",
      "18163        Brian Smith          R     71    185  0.000    0\n",
      "18164    Manny Delcarmen          R     74    205  0.000    0\n",
      "18165        Rudy Kallio          R     70    160  0.167    0\n",
      "18166          Ed Barney          L     70    178  0.224    0\n",
      "18167         Mike Joyce          R     74    193  0.429    0\n",
      "18168        Joe Blanton          R     75    245  0.108    0\n",
      "18169        Jeremy Hill          R     71    200  0.000    0\n",
      "18170      Aurelio Lopez          R     72    185  0.125    0\n",
      "18171         Jesse Hill          R     69    165  0.289    6\n",
      "18172         Rob Nelson          L     76    215  0.178    4\n",
      "18173       Roger Nelson          R     75    200  0.128    0\n",
      "18174       Fred Luderus          L     71    185  0.277   84\n",
      "18175           Ed Glenn          R     70    160  0.202    1\n",
      "18176           Ed Glenn          R                0.067    0\n",
      "\n",
      "[18177 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(baseball_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_h = baseball_data[baseball_data['handedness'] == 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_h = baseball_data[baseball_data['handedness'] == 'L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, (-9.9357022262420944, 3.8102742258887383e-23))\n"
     ]
    }
   ],
   "source": [
    "# Ignoring NaN handness\n",
    "t_test_result = t_test_compare(right_h['avg'], left_h['avg'])\n",
    "print(t_test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric Test\n",
    "Statistical test that does not assume our data is drawn from anny particular underlying probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann-Whitney U Test (-Wilcoxan Test)\n",
    "This is a test of the null hypothesis that two populations are the same.\n",
    "\n",
    "Tests whether or not these samples came from the same population - but not necessarily which one has a higher mean or higher median or anything like that\n",
    "\n",
    "Because of this it is usually useful to report Mann-Whitney U Test results along with some other information (like the two samples means, or the sample medians...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannwhitneyuResult(statistic=22523894.5, pvalue=3.7307870396512496e-45)\n"
     ]
    }
   ],
   "source": [
    "# u: Mann-Whitney test statistic\n",
    "# p: one sided pvalue\n",
    "u_test = sps.mannwhitneyu(right_h['avg'], left_h['avg'])\n",
    "print(u_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "A branch of artificial intelligence focused on constructing systems that learn from large amounts of data to make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics vs. Machine Learning\n",
    "NOT MUCH\n",
    "\n",
    "* Statistics is focused on analyzing existing data, and drawing valid conclusions (care about how the data is collected and drawing conclusions about that existing data using probability models)\n",
    "* Machine Learning is focused on making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Machine Learning: Supervised and Unsupervised\n",
    "Data -> MODEL -> Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "Do not have any such training examples. Instead, we have a bunch of unlabeled data points and we are trying to understand the structure of the data, often by clustering similar data points together.\n",
    "* Trying to understand structure of data\n",
    "* Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Supervised Learning\n",
    " There are labeled inputs that we train the model on. Training the model means teaching the model what the correct answer looks like.\n",
    " * Have examples with input and output\n",
    " * Predict output for future\n",
    " * Classification\n",
    " * Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with Gradient Descent\n",
    "Can we write an equation that takes a bunch of info (e.g., height, weight, birth year, position) and predicts the number of home runs? Yeah, regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
